{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # to get \"unresolved\" packages\n",
    "sys.path.append(\"/Users/a1765262/opt/anaconda3/lib/python3.9/site-packages\")\n",
    "\n",
    "import pandas as pd # for data frames\n",
    "import os # for working directories\n",
    "import numpy as np # for numbers\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from corextopic import corextopic as ct # for topic model\n",
    "import string # for preprocessing\n",
    "import sklearn.feature_extraction.text # for vectoriser\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for sentiment\n",
    "import pickle\n",
    "import scipy\n",
    "from nltk import FreqDist\n",
    "import shifterator as sh\n",
    "import datetime\n",
    "# from readability import readability\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "sys.path.append(\"./Other\")\n",
    "import irulan\n",
    "\n",
    "channel_list = [\"ABC1\", \"Ch7\", \"Ch9\", \"Ch10\", \"SBS\", \"ABC24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in text data from 2022. This is split up by channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = pickle.load(open(\"./Data/2022_all_clean_split.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of words, and the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = pickle.load(open(\"./Data/2022_all_clean_split.pkl\", \"rb\"))\n",
    "\n",
    "for i, channel in enumerate(channel_list):    \n",
    "\n",
    "    text = \" \".join(text_split[i]).split()\n",
    "    print(f\"Total number of words ({channel}):\", len(text))\n",
    "    print(f\"Number of unique words ({channel}):\", len(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the sentiment of each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_lexicon = pickle.load(open('./Data/NRC_lexicon.pkl', 'rb'))\n",
    "mittens_lexicon = pickle.load(open('./Data/mittens_lexicon.pkl', 'rb'))\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = text_split[i]\n",
    "\n",
    "    s = np.mean(np.array([irulan.doc_sentiment(doc, mittens_lexicon) for doc in text]))\n",
    "    print(\"Mittens sentiment of \" + str(channel) + \": \" + str(s))\n",
    "\n",
    "    s = np.mean(np.array([irulan.doc_sentiment(doc, nrc_lexicon) for doc in text]))\n",
    "    print(\"NRC sentiment of \" + str(channel) + \": \" + str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the terms with the greatest difference in Tsallis entropy on each channel. Use the terms with the greatest difference in entropy to create word clouds. While we do this, we can also find the Shannon entropy of text from each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the colour of each word\n",
    "\n",
    "def word_col_labmt(word, lexicon, font_size, position, orientation, random_state = None, **kwargs):\n",
    "    \n",
    "    word_col = 'black'\n",
    "    \n",
    "    if word in lexicon.keys():\n",
    "        \n",
    "        if lexicon[word] > 0.2:\n",
    "            \n",
    "            word_col = 'green'\n",
    "        \n",
    "        elif lexicon[word] < -0.2:\n",
    "            \n",
    "            word_col = 'red'\n",
    "        \n",
    "    return word_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pickle.load(open(\"./Data/2022_all_clean_split.pkl\", \"rb\"))\n",
    "\n",
    "all_text_for_counts = all_text[0][:]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    all_text_for_counts.extend(all_text[i])\n",
    "\n",
    "all_text_for_counts = \" \".join(all_text_for_counts)\n",
    "\n",
    "all_counts = dict() # initialise dictionary\n",
    "# get frequency counts\n",
    "all_counts = FreqDist(all_text_for_counts.split())\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = \" \".join(all_text[i])\n",
    "\n",
    "    channel_counts = dict() # initialise dictionary\n",
    "    # get frequency counts\n",
    "    channel_counts = FreqDist(text.split())\n",
    "    \n",
    "    # calculate Tsallis entropy shifts\n",
    "    entropy_shift = sh.EntropyShift(type2freq_1=all_counts,\n",
    "                                    type2freq_2=channel_counts,\n",
    "                                    alpha = 0.3)\n",
    "    \n",
    "    shft_scores = entropy_shift.get_shift_scores()\n",
    "\n",
    "    counts_array = np.array(list(channel_counts.values()))\n",
    "\n",
    "    # get the entropy of each channel \n",
    "    print(scipy.stats.entropy(counts_array))\n",
    "\n",
    "# TODO include word cloud code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get plots for periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "daily_counts = np.zeros((6, 365))\n",
    "daily_dates = [datetime.date(2022, 1, 1) + datetime.timedelta(days = i) for i in range(365)]\n",
    "colours = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:pink']\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "\n",
    "    # get text and dates\n",
    "    text = text_split[i]\n",
    "    dates = pickle.load(open(\"./Data/all_dates_channel_split.pkl\", \"rb\"))[i]\n",
    "\n",
    "    # filter to just 2022\n",
    "    text = [t for d, t in zip(dates, text) if d.year == 2022]\n",
    "    dates = [d for d in dates if d.year == 2022]\n",
    "\n",
    "    # count the number of words on each day\n",
    "    for d, t in zip(dates, text):\n",
    "\n",
    "        k = (d-datetime.datetime(2022, 1, 1)).days\n",
    "\n",
    "        daily_counts[i, k] += len(t.split())\n",
    "\n",
    "    # plot\n",
    "    plt.plot(daily_dates, daily_counts[i], label = ch, color = colours[i])\n",
    "\n",
    "    plt.title('Daily word counts in 2022')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Word count')\n",
    "    plt.legend(bbox_to_anchor = (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of unique programs, and the number of genres in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channel_list:\n",
    "\n",
    "    data = pd.read_csv(f'./Data/{channel}_2022.csv')\n",
    "\n",
    "    print(f\"Number of individual programs ({channel}):\", len(set(data[\"program\"])))\n",
    "    print(f\"Number of genres ({channel}):\", len(set(data[\"genre\"])))\n",
    "    print(text.groupby(\"genre\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = pd.read_csv(f\"./Data/Yearly Data/{channel}_2022.csv\")\n",
    "\n",
    "    print(channel, text[\"program\"].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = pd.read_csv(f\"./Data/Yearly Data/{channel}_2022.csv\")\n",
    "\n",
    "    print(channel, text[\"genre\"].value_counts()[:5]/len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the sentiment of each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict_to_text = {}\n",
    "\n",
    "for ch in channel_list:\n",
    "\n",
    "    text = pd.read_csv(\"./Data/Yearly Data/\" + ch + \"_2022.csv\")\n",
    "\n",
    "    for genre, line in zip(text[\"genre\"], text[\"text\"]):\n",
    "\n",
    "            if genre not in genre_dict_to_text.keys():\n",
    "\n",
    "                genre_dict_to_text[genre] = [line]\n",
    "\n",
    "            else:\n",
    "\n",
    "                genre_dict_to_text[genre].append(line)\n",
    "\n",
    "for genre, text_list in genre_dict_to_text.items():\n",
    "\n",
    "    mittens_sentiment = 0\n",
    "    nrc_sentiment = 0\n",
    "    k = 0\n",
    "\n",
    "    for doc in text_list:\n",
    "    \n",
    "        doc = str(doc).replace(\"'\", \"\").lower()\n",
    "\n",
    "        for c in string.punctuation:\n",
    "\n",
    "            doc = str(doc).replace(c, \"\").lower()\n",
    "\n",
    "        mittens_sentiment += irulan.doc_sentiment(doc, mittens_lexicon)\n",
    "        nrc_sentiment += irulan.doc_sentiment(doc, nrc_lexicon)\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    mittens_sentiment /= np.max([1, k])\n",
    "    nrc_sentiment /= np.max([1, k])\n",
    "\n",
    "    print(f'{genre} mittens sentiment: {mittens_sentiment}')\n",
    "    print(f'{genre} NRC sentiment: {nrc_sentiment}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
