{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # to get \"unresolved\" packages\n",
    "sys.path.append(\"/Users/a1765262/opt/anaconda3/lib/python3.9/site-packages\")\n",
    "\n",
    "import pandas as pd # for data frames\n",
    "import os # for working directories\n",
    "import numpy as np # for numbers\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from corextopic import corextopic as ct # for topic model\n",
    "import string # for preprocessing\n",
    "import sklearn.feature_extraction.text # for vectoriser\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for sentiment\n",
    "import pickle\n",
    "import scipy\n",
    "from nltk import FreqDist\n",
    "import shifterator as sh\n",
    "import datetime\n",
    "# from readability import readability\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "sys.path.append(\"./Other\")\n",
    "import irulan\n",
    "\n",
    "channel_list = [\"ABC1\", \"Ch7\", \"Ch9\", \"Ch10\", \"SBS\", \"ABC24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in text data from 2022. This is split up by channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = pickle.load(open(\"./Data/2022_all_clean_split.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of words, and the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pickle.load(open(\"./Data/2022_all_clean_split.pkl\", \"rb\"))\n",
    "\n",
    "for channel in channel_list:    \n",
    "\n",
    "    text = \" \".join(text[channel]).split()\n",
    "    print(f\"Total number of words ({channel}):\", len(text))\n",
    "    print(f\"Number of unique words ({channel}):\", len(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the sentiment of each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_lexicon = pickle.load(open('./Data/NRC_lexicon.pkl', 'rb'))\n",
    "mittens_lexicon = pickle.load(open('./Data/mittens_lexicon.pkl', 'rb'))\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = text_split[i]\n",
    "\n",
    "    s = np.mean(np.array([irulan.doc_sentiment(doc, mittens_lexicon) for doc in text]))\n",
    "    print(\"Mittens sentiment of \" + str(channel) + \": \" + str(s))\n",
    "\n",
    "    s = np.mean(np.array([irulan.doc_sentiment(doc, nrc_lexicon) for doc in text]))\n",
    "    print(\"NRC sentiment of \" + str(channel) + \": \" + str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the terms with the greatest difference in Tsallis entropy on each channel. Use the terms with the greatest difference in entropy to create word clouds. While we do this, we can also find the Shannon entropy of text from each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the colour of each word\n",
    "\n",
    "def word_col_labmt(word, lexicon, font_size, position, orientation, random_state = None, **kwargs):\n",
    "    \n",
    "    word_col = 'black'\n",
    "    \n",
    "    if word in lexicon.keys():\n",
    "        \n",
    "        if lexicon[word] > 0.2:\n",
    "            \n",
    "            word_col = 'green'\n",
    "        \n",
    "        elif lexicon[word] < -0.2:\n",
    "            \n",
    "            word_col = 'red'\n",
    "        \n",
    "    return word_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pickle.load(open(\"./Data/2022_all_clean_split.pkl\", \"rb\"))\n",
    "\n",
    "all_text_for_counts = all_text[0][:]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    all_text_for_counts.extend(all_text[i])\n",
    "\n",
    "all_text_for_counts = \" \".join(all_text_for_counts)\n",
    "\n",
    "all_counts = dict() # initialise dictionary\n",
    "# get frequency counts\n",
    "all_counts = FreqDist(all_text_for_counts.split())\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = \" \".join(all_text[i])\n",
    "\n",
    "    channel_counts = dict() # initialise dictionary\n",
    "    # get frequency counts\n",
    "    channel_counts = FreqDist(text.split())\n",
    "    \n",
    "    # calculate Tsallis entropy shifts\n",
    "    entropy_shift = sh.EntropyShift(type2freq_1=all_counts,\n",
    "                                    type2freq_2=channel_counts,\n",
    "                                    alpha = 0.3)\n",
    "    \n",
    "    shft_scores = entropy_shift.get_shift_scores()\n",
    "\n",
    "    counts_array = np.array(list(channel_counts.values()))\n",
    "\n",
    "    # get the entropy of each channel \n",
    "    print(scipy.stats.entropy(counts_array))\n",
    "\n",
    "# TODO include word cloud code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get plots for periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of unique programs, and the number of genres in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channel_list:\n",
    "\n",
    "    data = pd.read_csv(f'./Data/{channel}_2022.csv')\n",
    "\n",
    "    print(f\"Number of individual programs ({channel}):\", len(set(data[\"program\"])))\n",
    "    print(f\"Number of genres ({channel}):\", len(set(data[\"genre\"])))\n",
    "    print(text.groupby(\"genre\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = pd.read_csv(f\"./Data/Yearly Data/{channel}_2022.csv\")\n",
    "\n",
    "    print(channel, text[\"program\"].value_counts()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    text = pd.read_csv(f\"./Data/Yearly Data/{channel}_2022.csv\")\n",
    "\n",
    "    print(channel, text[\"genre\"].value_counts()[:5]/len(text))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
