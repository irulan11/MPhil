{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is (most of) the code required to obtain results in the Sentiment Analysis chapter. Code to produce word shifts and word shift plots is given in  `Word Shift Graphs.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 08:48:00.685137: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "import shifterator as sh\n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens import Mittens\n",
    "import scipy\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "import irulan\n",
    "\n",
    "# list of channels\n",
    "channel_list = ['ABC1', 'Ch7', 'Ch9', 'Ch10', 'SBS', 'ABC24']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text, lexicon):\n",
    "\n",
    "    sentiment_scores = list()\n",
    "\n",
    "    for ch, ch_text in enumerate(text):\n",
    "\n",
    "        sentiment_scores.append([])\n",
    "\n",
    "        for i, doc in enumerate(ch_text):\n",
    "\n",
    "            s = 0\n",
    "            k = 0\n",
    "\n",
    "            for word in doc.split():\n",
    "\n",
    "                if word in lexicon.keys():\n",
    "                    s += lexicon[word]\n",
    "                    k += 1\n",
    "\n",
    "            sentiment_scores[ch].append(s/np.max([k, 1]))\n",
    "\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"all_text_split.pkl\", \"rb\"))\n",
    "nrc_lexicon = pickle.load(open('nrc_lexicon.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"all_dates_split.pkl\", \"rb\"))\n",
    "\n",
    "# get sentiment scores from text\n",
    "sentiment_scores = get_sentiment(text, nrc_lexicon)\n",
    "\n",
    "# make plot\n",
    "a = 20000\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(sentiment_scores[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title(\"Sentiment of each channel over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"all_news_text_split.pkl\", \"rb\"))\n",
    "nrc_lexicon = pickle.load(open('nrc_lexicon.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"all_news_dates_split.pkl\", \"rb\"))\n",
    "\n",
    "# get sentiment scores from text\n",
    "sentiment_scores = get_sentiment(text, nrc_lexicon)\n",
    "\n",
    "# make plot\n",
    "a = 20000\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(sentiment_scores[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title(\"Sentiment of each channel over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune GloVe embeddings with Mittens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "text = pickle.load(open(\"all_news_text.pkl\", \"rb\")) # not split by channel\n",
    "glove = 'glove.6B.300d.txt'\n",
    "\n",
    "# fine-tune GloVe embeddings with Mittens\n",
    "new_glove = irulan.train_mittens(text, glove)\n",
    "pickle.dump(new_glove, open('mittens_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sentiment scores to create lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_glove = pickle.load(open('mittens_model.pkl', 'rb'))\n",
    "\n",
    "mittens_lexicon = dict()\n",
    "\n",
    "# loop through each word with embeddings and calculate a sentiment score for each\n",
    "for word in new_glove.keys():\n",
    "\n",
    "    mittens_lexicon[word] = irulan.glove_sentiment(word)\n",
    "\n",
    "pickle.dump(mittens_lexicon, open('mittens_lexicon', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the robustness of the Mittens lexicon. Calculate the Pearson correlation of sentiment values for 100 runs of Mittens embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of iterations at 100\n",
    "iterations = 100\n",
    "\n",
    "# load the original lexicon to compare with\n",
    "original_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "text = pickle.load(open(\"all_news_text.pkl\", \"rb\"))\n",
    "glove = 'glove.6B.300d.txt'\n",
    "\n",
    "pearsons = np.zeros(iterations)\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # train a new Mittens model each iteration\n",
    "    new_glove = irulan.train_mittens(text, glove, seed = i)\n",
    "\n",
    "    new_lexicon = dict()\n",
    "    # loop through each word with embeddings and calculate a sentiment score for each\n",
    "    for word in new_glove.keys():\n",
    "\n",
    "        # add sentiment scores for each word to an array\n",
    "        sentiments = np.append(sentiments, [[original_lexicon[word], irulan.glove_sentiment(word)]], axis = 0)\n",
    "\n",
    "    # calculate the pearson correlation of the original and new sentiments\n",
    "    pearsons[i] = scipy.stats.pearsonr(sentiments[:, 0], sentiments[:, 1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Pearson correlation for subsampled models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of iterations at 30\n",
    "iterations = 30\n",
    "\n",
    "# load the original lexicon to compare with\n",
    "original_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "text = pickle.load(open(\"all_news_text.pkl\", \"rb\"))\n",
    "glove = 'glove.6B.300d.txt'\n",
    "\n",
    "pearsons = np.zeros(iterations)\n",
    "\n",
    "# loop through each value of p 30 times\n",
    "for i in range(iterations*9):\n",
    "\n",
    "    # proportions from 0.1 to 0.9\n",
    "    p = ((i % 9) + 1) / 10\n",
    "    np.random.seed(seed = i)\n",
    "\n",
    "    # choose a proportion of the text to train the model on\n",
    "    samples = np.random.randint(0, len(text), int(len(text)*p))\n",
    "    text = [text[k] for k in samples]\n",
    "\n",
    "    # train a new Mittens model each iteration\n",
    "    new_glove = irulan.train_mittens(text, glove, seed = i)\n",
    "\n",
    "    new_lexicon = dict()\n",
    "    # loop through each word with embeddings and calculate a sentiment score for each\n",
    "    for word in new_glove.keys():\n",
    "\n",
    "        # add sentiment scores for each word to an array\n",
    "        sentiments = np.append(sentiments, [[original_lexicon[word], irulan.glove_sentiment(word)]], axis = 0)\n",
    "\n",
    "    # calculate the pearson correlation of the original and new sentiments\n",
    "    pearsons[i] = scipy.stats.pearsonr(sentiments[:, 0], sentiments[:, 1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Mittens lexicon with the NRC-VAD lexicon. Make an interactive plot and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lexicons\n",
    "nrc_lexicon = pickle.load(open('nrc_lexicon.pkl', 'rb'))\n",
    "mittens_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "\n",
    "sentiments = np.empty((0, 2))\n",
    "word_list = []\n",
    "\n",
    "# create lexicons of text in common\n",
    "nrc_filtered = dict()\n",
    "mittens_filtered = dict()\n",
    "\n",
    "# loop through the words and check whether they appear in both lists\n",
    "for word in nrc_lexicon.keys():\n",
    "\n",
    "    if word in mittens_lexicon.keys():\n",
    "\n",
    "        # add sentiment scores to an array if they are in both sentiment dictionaries\n",
    "        sentiments = np.append(sentiments, [[nrc_lexicon[word], mittens_lexicon[word]]], axis = 0)\n",
    "        word_list.append(word) # add words to a list for plotting\n",
    "\n",
    "        nrc_filtered[word] = nrc_lexicon[word]\n",
    "        mittens_filtered[word] = mittens_lexicon[word]\n",
    "\n",
    "pickle.dump(nrc_filtered, open('nrc_lexicon_filtered.pkl', 'wb'))\n",
    "pickle.dump(mittens_filtered, open('mittens_lexicon_filtered.pkl', 'wb'))\n",
    "\n",
    "x = np.linspace(-1.5,1.5,100)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=sentiments[:, 0],\n",
    "    y=sentiments[:, 1],\n",
    "    hovertext = word_list,\n",
    "    name=\"Sentiment Score\",\n",
    "    mode = 'markers')\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(trace)\n",
    "fig['layout'].update(xaxis_title = 'NRC lexicon', yaxis_title = 'Mittens lexicon', width = 800)\n",
    "fig.update_yaxes(\n",
    "scaleanchor=\"x\",\n",
    "scaleratio=1)\n",
    "iplot(fig)\n",
    "\n",
    "# calculate pearson correlation\n",
    "pearson = scipy.stats.pearsonr(sentiments[:, 0], sentiments[:, 1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the words in common between nrc and mittens lexicon\n",
    "\n",
    "# load lexicons that include only the words in common\n",
    "nrc_filtered = pickle.load(open('nrc_lexicon_filtered.pkl', 'rb'))\n",
    "mittens_filtered = pickle.load(open('mittens_lexicon_filtered.pkl', 'rb'))\n",
    "\n",
    "# plot histograms of the sentiments\n",
    "plt.hist(mittens_filtered, bins = 50, alpha = 0.5, label = \"Mittens\")\n",
    "plt.hist(nrc_filtered, bins = 20, alpha = 0.5, label = \"NRC\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another sentiment analysis (news text with Mittens lexicon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"all_news_text_split.pkl\", \"rb\"))\n",
    "mittens_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"all_news_dates_split.pkl\", \"rb\"))\n",
    "\n",
    "# get sentiment scores from text\n",
    "sentiment_scores = get_sentiment(text, mittens_lexicon)\n",
    "\n",
    "# make plot\n",
    "a = 20000\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(sentiment_scores[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title(\"Sentiment of each channel over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it bleeds, it leads: investigate the sentiment of 5-minute intervals of news text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Political sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get political text\n",
    "\n",
    "# read in original data\n",
    "text = pickle.load(open(\"all_text_split_60.pkl\", \"rb\"))\n",
    "dates = pickle.load(open(\"all_dates_split_60.pkl\", \"rb\"))\n",
    "pyx = pickle.load(open('political_probabilities.pkl', 'rb')) # from topic modelling\n",
    "\n",
    "# initialise\n",
    "liberal_text = [[]]*6\n",
    "liberal_dates = [[]]*6\n",
    "labor_text = [[]]*6\n",
    "labor_dates = [[]]*6\n",
    "\n",
    "# filter text and dates to only those with probability of the respective political \n",
    "# topic greater than or equal to 0.5\n",
    "for t, d, p in zip(text, dates, pyx):\n",
    "    liberal_text = [t[k] for k in range(len(t)) if p[k, 0]>=0.5]\n",
    "    liberal_dates = [d[k] for k in range(len(d)) if p[k, 0]>=0.5]\n",
    "    labor_text = [t[k] for k in range(len(t)) if p[k, 1]>=0.5]\n",
    "    labor_dates = [d[k] for k in range(len(d)) if p[k, 1]>=0.5]\n",
    "\n",
    "# save\n",
    "pickle.dump(liberal_text, open('liberal_text.pkl', 'wb'))\n",
    "pickle.dump(liberal_dates, open('liberal_dates.pkl', 'wb'))\n",
    "pickle.dump(labor_text, open('labor_text.pkl', 'wb'))\n",
    "pickle.dump(labor_dates, open('labor_dates.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sentiment of political text and plot\n",
    "\n",
    "# read in data\n",
    "liberal_text = pickle.load(open('liberal_text.pkl', 'rb'))\n",
    "liberal_dates = pickle.load(open('liberal_dates.pkl', 'rb'))\n",
    "labor_text = pickle.load(open('labor_text.pkl', 'rb'))\n",
    "labor_dates = pickle.load(open('labor_dates.pkl', 'rb'))\n",
    "mittens_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "\n",
    "# calculate sentiment\n",
    "liberal_sentiment = get_sentiment(liberal_text)\n",
    "labor_sentiment = get_sentiment(labor_text)\n",
    "pickle.dump(liberal_sentiment, open('liberal_sentiment.pkl', 'wb'))\n",
    "pickle.dump(labor_sentiment, open('labor_sentiment.pkl', 'wb'))\n",
    "\n",
    "# liberal plot\n",
    "a = 20000\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(liberal_dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(liberal_sentiment[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title('Liberal Sentiment')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")\n",
    "\n",
    "# labor plot\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(labor_dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(labor_sentiment[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title('Labor Sentiment')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get daily values\n",
    "\n",
    "# load data\n",
    "mittens_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "pyx = pickle.load(open('political_probabilities.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"all_dates_split_60.pkl\", \"rb\"))\n",
    "\n",
    "# initialise\n",
    "liberal_daily_sentiments = [[[]]]\n",
    "labor_daily_sentiments = [[[]]]\n",
    "daily_dates = [[]]\n",
    "\n",
    "# loop through the channels\n",
    "for i, channel_dates, channel_pyx in zip(range(6), dates, pyx):\n",
    "    \n",
    "    # get the first date\n",
    "    d0 = channel_dates[0]\n",
    "    daily_dates[i] = [d0]\n",
    "    k = 0\n",
    "\n",
    "    # get the text for this channel\n",
    "    channel_text = pickle.load(open(\"all_text_split_60.pkl\", \"rb\"))[i]\n",
    "\n",
    "    # loop through documents in this channel\n",
    "    for d, t, p in zip(channel_dates, channel_text, channel_pyx):\n",
    "\n",
    "        # if it is a new day, add to the date list\n",
    "        if d.day != d0.day:\n",
    "            liberal_daily_sentiments[i].append([])\n",
    "            labor_daily_sentiments[i].append([])\n",
    "            daily_dates[i].append(d)\n",
    "            k += 1\n",
    "\n",
    "        # update the current day\n",
    "        d0 = d\n",
    "\n",
    "        # add to liberal or labor sentiment if p(y|x) is greater than or equal to 0.5\n",
    "        if p[0] >= 0.5:\n",
    "            liberal_daily_sentiments[i][k].append(irulan.doc_sentiment(t, mittens_lexicon))\n",
    "\n",
    "        if p[1] >= 0.5:\n",
    "            labor_daily_sentiments[i][k].append(irulan.doc_sentiment(t, mittens_lexicon))\n",
    "\n",
    "    # change of channel, add to lists\n",
    "    daily_dates.append([])\n",
    "    liberal_daily_sentiments.append([[]])\n",
    "    labor_daily_sentiments.append([[]])\n",
    "\n",
    "# initialise the daily averages\n",
    "lib_daily_averages = [[]]*6\n",
    "lab_daily_averages = [[]]*6\n",
    "\n",
    "# calculate averages\n",
    "for i in range(6):\n",
    "    lib_daily_averages[i] = [np.mean(lib) for lib in liberal_daily_sentiments[i]]\n",
    "    lab_daily_averages[i] = [np.mean(lab) for lab in labor_daily_sentiments[i]]\n",
    "\n",
    "# save\n",
    "pickle.dump(lib_daily_averages, open('daily_average_liberal_sentiment.pkl', 'wb'))\n",
    "pickle.dump(lab_daily_averages, open('daily_average_labor_sentiment.pkl', 'wb'))\n",
    "pickle.dump(daily_dates, open('daily_dates.pkl', 'wb'))\n",
    "\n",
    "# plot liberal daily averages\n",
    "a = 50\n",
    "for i, ch in enumerate(channel_list):\n",
    "\n",
    "    # remove some nans\n",
    "    lib_non_nan = [l for l in lib_daily_averages[i] if not np.isnan(l)]\n",
    "    daily_dates_non_nan = [d for d, l in zip(daily_dates[i], lib_daily_averages[i]) if not np.isnan(l)]\n",
    "\n",
    "    # plot\n",
    "    plt.plot(daily_dates_non_nan[int(a/2):-int(a/2)+1], irulan.moving_average(lib_non_nan, a), label = ch)\n",
    "    plt.title(\"The average daily sentiment of Liberal text\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sentiment\")\n",
    "    plt.legend(bbox_to_anchor = ((1,1)))\n",
    "\n",
    "# plot labor daily averages\n",
    "for i, ch in enumerate(channel_list):\n",
    "\n",
    "    # remove some nans\n",
    "    lab_non_nan = [l for l in lab_daily_averages[i] if not np.isnan(l)]\n",
    "    daily_dates_non_nan = [d for d, l in zip(daily_dates[i], lab_daily_averages[i]) if not np.isnan(l)]\n",
    "\n",
    "    # plot\n",
    "    plt.plot(daily_dates_non_nan[int(a/2):-int(a/2)+1], irulan.moving_average(lab_non_nan, a), label = ch)\n",
    "    plt.title(\"The average daily sentiment of Labor text\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sentiment\")\n",
    "    plt.legend(bbox_to_anchor = ((1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"all_text_split_60.pkl\", \"rb\"))\n",
    "dates = pickle.load(open(\"all_dates_split_60.pkl\", \"rb\"))\n",
    "mittens_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "\n",
    "# get Mittens sentiment scores for all text\n",
    "sentiment_scores = get_sentiment(text, mittens_lexicon)\n",
    "pickle.dump(sentiment_scores, open('all_sentiments_60.pkl', 'wb'))\n",
    "\n",
    "# get topic probabilities\n",
    "pyx = pickle.load(open('political_probabilities.pkl', 'rb'))\n",
    "\n",
    "# calculate and save weighted sentiment\n",
    "liberal_weighted_sentiment = [sentiment_scores[i]*pyx[i][:, 0] for i in range(len(channel_list))]\n",
    "labor_weighted_sentiment = [sentiment_scores[i]*pyx[i][:, 1] for i in range(len(channel_list))]\n",
    "pickle.dump(liberal_weighted_sentiment, open('liberal_weighted_sentiment.pkl', 'wb'))\n",
    "pickle.dump(labor_weighted_sentiment, open('labor_weighted_sentiment.pkl', 'wb'))\n",
    "\n",
    "# make liberal plot\n",
    "a = 20000\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(liberal_weighted_sentiment[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title('Weighted sentiment of Liberal text')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")\n",
    "\n",
    "# make laborplot\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(labor_weighted_sentiment[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title('Weighted sentiment of Labor text')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment bias: mean sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liberal_sentiment = pickle.load(open('liberal_sentiment.pkl', 'rb'))\n",
    "labor_sentiment = pickle.load(open('labor_sentiment.pkl', 'rb'))\n",
    "\n",
    "liberal_average_sentiment = np.mean(liberal_sentiment)\n",
    "labor_average_sentiment = np.mean(labor_sentiment)\n",
    "\n",
    "print('Liberal mean sentiment:', liberal_average_sentiment)\n",
    "print('Labor mean sentiment:', labor_average_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment bias: word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = 'glove.6B.300d.txt'\n",
    "\n",
    "# create list of Liberal/Labor words\n",
    "liberal_words = ['liberal', 'scott', 'morrison', 'malcolm', 'turnbull']\n",
    "labor_words = ['labor', 'anthony', 'albanese', 'bill', 'shorten']\n",
    "\n",
    "# create a Mittens model trained on each channel\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    # load text from one channel\n",
    "    text = pickle.load(open(\"all_text_split.pkl\", \"rb\"))[i]\n",
    "\n",
    "    # train a Mittens model on text from this channel \n",
    "    new_glove = irulan.train_mittens(text, glove)\n",
    "    pickle.dump(new_glove, open(f'mittens_model_{channel}.pkl', 'wb'))\n",
    "\n",
    "    # find the sentiment values for Liberal and Labor terms\n",
    "    liberal_sentiment = 0\n",
    "    labor_sentiment = 0\n",
    "\n",
    "    for word in liberal_words:\n",
    "        liberal_sentiment += irulan.glove_sentiment(word)\n",
    "\n",
    "    for word in labor_words:\n",
    "        labor_sentiment += irulan.glove_sentiment(word)\n",
    "\n",
    "    print('Liberal sentiment:', liberal_sentiment/5)\n",
    "    print('Labor sentiment:', labor_sentiment/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sentiment to the bias measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weighted sentiment scores calculated previously\n",
    "liberal_weighted_sentiment = pickle.load(open('liberal_weighted_sentiment.pkl', 'rb'))\n",
    "labor_weighted_sentiment = pickle.load(open('labor_weighted_sentiment.pkl', 'rb'))\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "    mod = np.mean(np.array(liberal_weighted_sentiment[i]) - np.array(labor_weighted_sentiment[i]))\n",
    "\n",
    "    print(f'MOD for {channel}:', mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with polling and election data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually scraped, inserted into an excel spreadsheet and saved as a csv\n",
    "polls = pd.read_csv('opinion_polls.csv')\n",
    "\n",
    "# load other data\n",
    "liberal_weighted_sentiment = pickle.load(open('liberal_weighted_sentiment.pkl', 'rb'))\n",
    "labor_weighted_sentiment = pickle.load(open('labor_weighted_sentiment.pkl', 'rb'))\n",
    "dates = pickle.load(open('all_dates.pkl', 'rb'))\n",
    "\n",
    "# create one long array, rather than separated by channel\n",
    "dates = [date for channel in dates for date in channel]\n",
    "liberal_weighted_sentiment = [sentiment for channel in liberal_weighted_sentiment for sentiment in channel]\n",
    "liberal_weighted_sentiment = np.array([x for _, x in sorted(zip(dates, liberal_weighted_sentiment), key=lambda pair: pair[0])])\n",
    "labor_weighted_sentiment = [sentiment for channel in labor_weighted_sentiment for sentiment in channel]\n",
    "labor_weighted_sentiment = np.array([x for _, x in sorted(zip(dates, labor_weighted_sentiment), key=lambda pair: pair[0])])\n",
    "dates = sorted(dates)\n",
    "\n",
    "dates = [datetime.datetime.fromtimestamp(d) for d in dates]\n",
    "\n",
    "# plot the weighted sentiments\n",
    "a = 300000\n",
    "plt.plot(10*irulan.moving_average(np.array(liberal_weighted_sentiment) - np.array(labor_weighted_sentiment), a), dates[int(a/2):-int(a/2)+1], color = 'tab:blue', alpha = 0.4)\n",
    "\n",
    "# plot the polling data\n",
    "a = 4\n",
    "\n",
    "# make sure to remove NaN rows \n",
    "liberal_polls = np.array([float(p) for p in polls[\"LNP\"] if str(p)[0] == '0'])\n",
    "labor_polls = np.array([float(p) for p in polls[\"ALP\"] if str(p)[0] == '0'])\n",
    "poll_dates = [datetime.datetime.fromisoformat(d) for d, p in zip(polls[\"DATE\"], polls[\"ALP\"]) if str(p)[0] == '0']\n",
    "\n",
    "# plot polling data\n",
    "plt.plot(irulan.moving_average(liberal_polls-labor_polls, a), poll_dates[int(a/2):-int(a/2)+1], color = 'black', alpha = 0.8)\n",
    "plt.ylabel('Difference in polling percentage')\n",
    "plt.xlabel('Date')\n",
    "plt.plot([0, 0], [datetime.datetime(2015, 1, 1), datetime.datetime(2023, 1, 1)], color = \"black\", ls = \"dashed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
