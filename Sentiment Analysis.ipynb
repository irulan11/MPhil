{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is (most of) the code required to obtain results in the Sentiment Analysis chapter. Code to produce word shifts and word shift plots is given in  `Word Shift Graphs.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "import sys\n",
    "sys.path.append(\"/Users/a1765262/opt/anaconda3/lib/python3.9/site-packages\") # (for VS code sklearn)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint # to print json files nicely -- cf. print()\n",
    "# from spacytextblob.spacytextblob import SpacyTextBlob # need to pip install spacytextblob\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams, FreqDist\n",
    "import shifterator as sh\n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens import Mittens\n",
    "\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "sys.path.append(\"./Other\")\n",
    "import irulan # useful things I've done\n",
    "\n",
    "#vectorising channels\n",
    "# for the text\n",
    "channel_list = ['ABC1', 'Ch7', 'Ch9', 'Ch10', 'SBS', 'ABC24']\n",
    "channel_codes = {'ABC1':\"561\",'Ch10':\"1589\",'Ch9':\"1072\",'Ch7':\"1328\",'SBS':\"785\", 'ABC24':\"560\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text, lexicon):\n",
    "\n",
    "    sentiment_scores = list()\n",
    "\n",
    "    for ch, ch_text in enumerate(text):\n",
    "\n",
    "        sentiment_scores.append([])\n",
    "\n",
    "        for i, doc in enumerate(ch_text):\n",
    "\n",
    "            s = 0\n",
    "            k = 0\n",
    "\n",
    "            for word in doc.split():\n",
    "\n",
    "                if word in lexicon.keys():\n",
    "                    s += lexicon[word]\n",
    "                    k += 1\n",
    "\n",
    "            sentiment_scores[ch].append(s/np.max([k, 1]))\n",
    "\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"all_text_clean_channel_split.pkl\", \"rb\"))\n",
    "nrc_lexicon = pickle.load(open('nrc_lexicon.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"all_dates_clean_channel_split.pkl\", \"rb\"))\n",
    "\n",
    "# get sentiment scores from text\n",
    "sentiment_scores = get_sentiment(text, nrc_lexicon)\n",
    "\n",
    "# make plot\n",
    "a = 20000\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(sentiment_scores[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title(\"Sentiment of each channel over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"news_text_channel_split.pkl\", \"rb\"))\n",
    "nrc_lexicon = pickle.load(open('nrc_lexicon.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"news_dates_channel_split.pkl\", \"rb\"))\n",
    "\n",
    "# get sentiment scores from text\n",
    "sentiment_scores = get_sentiment(text, nrc_lexicon)\n",
    "\n",
    "# make plot\n",
    "a = 20000\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(sentiment_scores[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title(\"Sentiment of each channel over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune GloVe embeddings with Mittens. Some code adapted from 'https://github.com/roamanalytics/mittens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "text = pickle.load(open(\"all_news_text.pkl\", \"rb\"))\n",
    "\n",
    "# get vocabulary\n",
    "words = \" \".join(text).split()\n",
    "counter = Counter(words)\n",
    "vocab = list(dict(counter.most_common(30000)).keys())\n",
    "\n",
    "del words, counter\n",
    "\n",
    "# get pre-trained model\n",
    "with open('glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "    pre_glove = {line[0]: np.array(list(map(float, line[1:])))\n",
    "            for line in reader}\n",
    "\n",
    "# create matrix for mittens\n",
    "cv = CountVectorizer(ngram_range=(1,1), vocabulary=vocab)\n",
    "X = cv.fit_transform(text)\n",
    "\n",
    "del text\n",
    "\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "coocc_ar = Xc.toarray()\n",
    "\n",
    "del Xc, X\n",
    "\n",
    "# create mittens model\n",
    "mittens_model = Mittens(n=300, max_iter=2000)\n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc_ar,\n",
    "    vocab=vocab, \n",
    "    initial_embedding_dict= pre_glove)\n",
    "\n",
    "del pre_glove, coocc_ar, mittens_model\n",
    "\n",
    "new_glove = dict(zip(vocab, new_embeddings))\n",
    "del vocab, new_embeddings\n",
    "\n",
    "pickle.dump(new_glove, open('mittens_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sentiment scores to create lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_glove = pickle.load(open('mittens_model.pkl', 'wb'))\n",
    "\n",
    "mittens_lexicon = dict()\n",
    "\n",
    "# loop through each word with embeddings and calculate a sentiment score for each\n",
    "for word in new_glove.keys():\n",
    "\n",
    "    mittens_lexicon[word] = irulan.glove_sentiment(word)\n",
    "\n",
    "pickle.dump(mittens_lexicon, open('mittens_lexicon', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the robustness of the Mittens lexicon. Calculate the Pearson correlation of sentiment values for 100 runs of Mittens embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Pearson correlation for subsampled models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Mittens lexicon with the NRC-VAD lexicon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another sentiment analysis (news text with Mittens lexicon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = pickle.load(open(\"news_text_channel_split.pkl\", \"rb\"))\n",
    "mittens_lexicon = pickle.load(open('mittens_lexicon.pkl', 'rb'))\n",
    "dates = pickle.load(open(\"news_dates_channel_split.pkl\", \"rb\"))\n",
    "\n",
    "# get sentiment scores from text\n",
    "sentiment_scores = get_sentiment(text, mittens_lexicon)\n",
    "\n",
    "# make plot\n",
    "a = 20000\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    plt.plot(dates[i][int(a/2):-int(a/2)+1], irulan.moving_average(sentiment_scores[i], a), label = ch)\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1))\n",
    "plt.title(\"Sentiment of each channel over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it bleeds, it leads: investigate the sentiment of 5-minute intervals of news text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Political sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment bias: mean sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment bias: word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sentiment to the bias measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with polling and election data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
