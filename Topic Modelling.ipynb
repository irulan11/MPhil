{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # to get \"unresolved\" packages\n",
    "sys.path.append(\"/Users/a1765262/opt/anaconda3/lib/python3.9/site-packages\")\n",
    "\n",
    "import pandas as pd # for data frames\n",
    "import os # for working directories\n",
    "import numpy as np # for numbers\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from corextopic import corextopic as ct # for topic model\n",
    "import string # for preprocessing\n",
    "import sklearn.feature_extraction.text # for vectoriser\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for sentiment\n",
    "import pickle\n",
    "import scipy\n",
    "from nltk import FreqDist\n",
    "import shifterator as sh\n",
    "import scipy.stats as st\n",
    "import datetime\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "import irulan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "channel_list = [\"ABC1\", \"Ch7\", \"Ch9\", \"Ch10\", \"SBS\", \"ABC24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a topic model anchored on 'sport' for different document lengths. Plot the number of words in each document for 5-minute and program lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector of seconds that we will look at (9999 seconds represents the program split)\n",
    "seconds = [10, 60, 120, 300, 600, 1200, 1800, 9999]\n",
    "\n",
    "abc1_2022 = pd.read_csv(\"ABC1_2022.csv\")\n",
    "\n",
    "for i, s in enumerate(seconds):\n",
    "\n",
    "    # clean text so that documents are s seconds long\n",
    "    text = irulan.clean_split_mins(abc1_2022, s)\n",
    "\n",
    "    # train topic model\n",
    "    _, tm = irulan.tm(text, anchors = ['sport'], num_topics= 40)\n",
    "\n",
    "    print(tm.get_topics()[0])\n",
    "\n",
    "    # if we have split into 5-minute or program-length intervals, make histogram\n",
    "    if s in [300, 9999]:\n",
    "        plt.hist([len(doc.split()) for doc in text], bins = 50)\n",
    "        plt.title(\"The number of words contained in each program\")\n",
    "        plt.xlabel(\"Number of words\")\n",
    "        plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total correlation of topic models with the number of topics adjusted from 10 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pickle.load(open(\"2022_text_split.pkl\", \"rb\"))\n",
    "\n",
    "# make the topic model\n",
    "n_topics = np.array(range(10, 110, 10))\n",
    "iterations = 30\n",
    "tcs = np.empty((len(n_topics), iterations))\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "for k, n in enumerate(n_topics):\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # train the topic model\n",
    "        tm = ct.Corex(n_hidden = n, words = words, max_iter = 400, seed = i) \n",
    "        tm.fit(matrix, words = words, anchor_strength = 10)\n",
    "\n",
    "        tcs[k, i] = tm.tc\n",
    "\n",
    "        # save the topic model for later\n",
    "        pickle.dump(open(f'./{n}_topics_{i}.pkl', 'wb'))\n",
    "\n",
    "        print(f'Value {i} for {n} topics is: {tm.tc}')\n",
    "        del tm\n",
    "\n",
    "# make violin plot\n",
    "plt.violinplot(tcs, positions = np.array(range(10, 110, 10)), widths = 5)\n",
    "plt.title(\"The total correlation of topic models on 2022 data adjusting the number of topics\")\n",
    "plt.ylabel(\"Total correlation\")\n",
    "plt.xlabel(\"Number of topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson and Spearman similarities for topic models with an adjacent number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_similarity(tm1, tm2, similarity_type = 'pearson'):\n",
    "\n",
    "    assert similarity_type in ['pearson', 'spearman'], \"'similarity_type' should be 'pearson' or 'spearman'.\"\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    # get the p(y|x) terms\n",
    "    pyx1 = tm1.p_y_given_x\n",
    "    pyx2 = tm2.p_y_given_x\n",
    "\n",
    "    # get the words in the topics as a list\n",
    "    topics1 = [[line[k][0] for k in range(len(line))] for line in tm1.get_topics(n_words = 100)]\n",
    "    topics2 = [[line[k][0] for k in range(len(line))] for line in tm2.get_topics(n_words = 100)]\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    # loop through and compare all topics\n",
    "    for i, t1 in enumerate(topics1):\n",
    "            for j, t2 in enumerate(topics2):\n",
    "\n",
    "                # if the number of words in common is greater than or equal to 5\n",
    "                if len(set(t1).intersection(set(t2)))>=5:\n",
    "\n",
    "                    # get the p(y|x) terms\n",
    "                    a = pyx1[:, i]\n",
    "                    b = pyx2[:, j]\n",
    "\n",
    "                    # calculate the pearson or spearman similarity of the p(y|x) terms\n",
    "                    if similarity_type == 'pearson':\n",
    "\n",
    "                        similarity_score += scipy.stats.pearsonr(a, b)[0]\n",
    "                        k += 1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        similarity_score += scipy.stats.spearmanr(a, b, nan_policy='omit')[0]\n",
    "                        k += 1\n",
    "    \n",
    "    return similarity_score/max([k, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we already have 30 topic models for each number of topics...\n",
    "\n",
    "piecewise_comparisons = zip(range(10, 110, 10), range(20, 100, 10))\n",
    "iterations = 30\n",
    "pearson_matrix = np.identity(10)*iterations\n",
    "spearman_matrix = np.identity(10)*iterations\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    for n1, n2 in piecewise_comparisons:\n",
    "\n",
    "        # load both topic models\n",
    "        tm1 = pickle.load(open(f'./{n}_topics_{i}.pkl', 'rb'))\n",
    "        tm2 = pickle.load(open(f'./{n}_topics_{i}.pkl', 'rb'))\n",
    "\n",
    "        # calculate the pearson similarity between them\n",
    "        pearson = topic_similarity(tm1, tm2, 'pearson')\n",
    "        pearson_matrix[n1, n2] += pearson\n",
    "        pearson_matrix[n2, n1] += pearson\n",
    "\n",
    "        # calculate the spearman similarity between them\n",
    "        spearman = topic_similarity(tm1, tm2, 'spearman')\n",
    "        spearman_matrix[n1, n2] += spearman\n",
    "        spearman_matrix[n2, n1] += spearman\n",
    "\n",
    "# take the mean\n",
    "pearson_matrix /= iterations\n",
    "spearman_matrix /= iterations\n",
    "\n",
    "# plot as two heatmaps\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "\n",
    "        if i!=j and i+1!=j and i-1!=j:\n",
    "            spearman_matrix[i,j] = np.nan\n",
    "            pearson_matrix[i,j] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(spearman_matrix, vmin = 0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "\n",
    "        text = ax.text(j, i, round(spearman_matrix[i,j], 2), ha = 'center', va = 'center', color = 'w')\n",
    "\n",
    "ax.set_xticks(np.arange(10), labels = np.arange(10, 110, 10))\n",
    "ax.set_yticks(np.arange(10), labels = np.arange(10, 110, 10))\n",
    "ax.set_xlabel(\"Number of topics\")\n",
    "ax.set_ylabel(\"Number of topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate unsupervised topic models trained on 2022 data from ABC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open(\"2022_text_split.pkl\", \"rb\"))[0]\n",
    "dates = pickle.load(open(\"2022_dates_split.pkl\", \"rb\"))[0]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 5\n",
    "a = 5000\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # train the topic model\n",
    "    tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "    tm.fit(matrix, words = words)\n",
    "    print(tm.get_topics()[:5])\n",
    "\n",
    "    # sport will be topic 0 for all topic models except the second one\n",
    "    if i != 1:\n",
    "\n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:, 0], a), alpha = 0.8, label = i+1)\n",
    "\n",
    "    else:\n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:, 4], a), alpha = 0.8, label = i+1)\n",
    "\n",
    "    del tm\n",
    "\n",
    "# make the plot nice\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 30 topic models on ABC1 text with the anchor 'sport'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open('2022_text_split.pkl', 'rb'))[0]\n",
    "dates = pickle.load(open('2022_dates_split.pkl', 'rb'))[0]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 30\n",
    "a = 5000\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # train the topic model\n",
    "    tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "    tm.fit(matrix, words = words, anchors = ['sport'], anchor_strength = 10)\n",
    "    print(tm.get_topics()[0])\n",
    "    plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:, 0], a), color = 'black', alpha = 0.5)\n",
    "    del tm\n",
    "\n",
    "plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 30 topic models with anchors for each method of inference and calculate Pearson correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open('all_text_split.pkl', 'rb'))[0]\n",
    "dates = pickle.load(open('all_dates_split.pkl', 'rb'))[0]\n",
    "anchors = [['covid'],['sport'],['gardening'],['election']]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 30\n",
    "a = 5000\n",
    "correlations = np.zeros((2, 30))\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # ground truth\n",
    "    tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "    tm.fit(matrix, words = words, anchors = anchors, anchor_strength = 10)\n",
    "\n",
    "    # method 1\n",
    "    text_sampled = [text[i] for i in range(len(text)) if dates[i].year == 2022]\n",
    "    vec1, tm1 = irulan.tm(text_sampled, num_topics=40, anchors = anchors, seed = i)\n",
    "\n",
    "    # method 2\n",
    "    np.random.seed(seed = i)\n",
    "\n",
    "    # subsample proportion p\n",
    "    p = 0.2\n",
    "    ints = np.random.randint(0, len(text), int(len(text)*p))\n",
    "    text_sampled = [text[i] for i in ints]\n",
    "\n",
    "    vec2, tm2 = irulan.tm(text_sampled, num_topics=40, anchors = anchors, seed = i)\n",
    "\n",
    "    # make predictions from the sampled topic models\n",
    "    matrix1 = vec1.transform(text)\n",
    "    p1 = tm1.transform(matrix1, details = True)[0]\n",
    "\n",
    "    matrix2 = vec2.transform(text)\n",
    "    p2 = tm2.transform(matrix2, details = True)[0]\n",
    "\n",
    "    correlation1 = scipy.stats.pearsonr(tm.p_y_given_x[:, 0], p1[:, 0])\n",
    "    correlation2 = scipy.stats.pearsonr(tm.p_y_given_x[:, 0], p2[:, 0])\n",
    "\n",
    "    # plot the first iteration\n",
    "    if i == 0:\n",
    "        plt.plot(dates[int(a/2):-int(a/2)+1], irulan.moving_average(tm.p_y_given_x[:, 0], a), label = 'Ground truth')\n",
    "        plt.plot(dates[int(a/2):-int(a/2)+1], irulan.moving_average(p1[:, 0], a), label = 'Method 1')\n",
    "        plt.plot(dates[int(a/2):-int(a/2)+1], irulan.moving_average(p2[:, 0], a), label = 'Method 2')\n",
    "\n",
    "        for k, anchor in enumerate(anchors[0]):\n",
    "            print(f'{anchor} Method 1: {correlation1}')\n",
    "            print(f'{anchor} Method 2: {correlation2}')\n",
    "\n",
    "    correlations[0, i] = correlation1\n",
    "    correlations[1, i] = correlation2\n",
    "\n",
    "    correlation_means = np.mean(correlations, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different percentages to subsample with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open('2022_text_split.pkl', 'rb'))[0]\n",
    "dates = pickle.load(open('2022_dates_split.pkl', 'rb'))[0]\n",
    "anchors = [['covid'],['sport'],['gardening'],['election']]\n",
    "subsampled_proportions = [0.001, 0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 30\n",
    "a = 5000\n",
    "correlations = np.zeros((30, len(subsampled_proportions)))\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    for p in subsampled_proportions:\n",
    "\n",
    "        # ground truth\n",
    "        tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "        tm.fit(matrix, words = words, anchors = anchors, anchor_strength = 10)\n",
    "\n",
    "        # method 2\n",
    "        np.random.seed(seed = i)\n",
    "\n",
    "        # subsample proportion p\n",
    "        ints = np.random.randint(0, len(text), int(len(text)*p))\n",
    "        text_sampled = [text[i] for i in ints]\n",
    "\n",
    "        vec2, tm2 = irulan.tm(text_sampled, num_topics=40, anchors = anchors, seed = i)\n",
    "\n",
    "        # make predictions from the sampled topic model\n",
    "        matrix2 = vec2.transform(text)\n",
    "        p2 = tm2.transform(matrix2, details = True)[0]\n",
    "\n",
    "        correlations[i, p] = scipy.stats.pearsonr(tm.p_y_given_x[:, 0], p2[:, 0])\n",
    "\n",
    "# plot\n",
    "plt.violinplot(correlations);\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9], [\"0.1\", \"1\", \"2\", \"5\", \"10\", \"20\", \"40\", \"60\", \"80\"])\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.xlabel(\"Percentage of subsample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate unsupervised topic models trained on 2022 data for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # read in text and dates\n",
    "    text = pickle.load(open('2022_text_split.pkl', 'rb'))[i]\n",
    "    dates = pickle.load(open('2022_dates_split.pkl', 'rb'))[i]\n",
    "\n",
    "    # train topic model\n",
    "    _, tm = irulan.tm(text_sampled)\n",
    "\n",
    "    # print topics\n",
    "    print(tm.get_topics()[:5])\n",
    "\n",
    "    # plot\n",
    "    for j in range(5):\n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:,j], a))\n",
    "\n",
    "    plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare unsupervised topic models trained on each channel with Pearson similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "iterations = 30\n",
    "similarity_matrix = np.zeros((6, 6))\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    for j, channel1 in enumerate(channel_list):\n",
    "\n",
    "        # load in text and create topic model for channel 1\n",
    "        text1 = pickle.load(open('2022_text_split.pkl', 'rb'))[j]\n",
    "        vec1, tm1 = irulan.tm(text1, seed = i)\n",
    "\n",
    "        # get topics into a nice format\n",
    "        topics1 = [[line[k][0] for k in range(len(line))] for line in tm1.get_topics(n_words = 100)]\n",
    "\n",
    "        for k, channel2 in enumerate(channel_list):\n",
    "\n",
    "            # load in text and create topic model for channel 2\n",
    "            text2 = pickle.load(open('2022_text_split.pkl', 'rb'))[k]\n",
    "            vec2, tm2 = irulan.tm(text2, seed = i)\n",
    "\n",
    "            # get topics into a nice format\n",
    "            topics2 = [[line[k][0] for k in range(len(line))] for line in tm2.get_topics(n_words = 100)]\n",
    "\n",
    "            # predict values for channel 2s text using channel 1s topic model\n",
    "            matrix1 = vec1.transform(text2)\n",
    "            p1 = tm1.transform(matrix1, details = True)[0]\n",
    "\n",
    "            p2 = tm2.p_y_given_x\n",
    "\n",
    "            # initialise the similarity score\n",
    "            similarity_score = 0\n",
    "            sums = 0\n",
    "\n",
    "            # loop through topics \n",
    "            for m, t1 in enumerate(topics1):\n",
    "                for n, t2 in enumerate(topics2):\n",
    "\n",
    "                    # check if the topics share at least 5 words\n",
    "                    if len(set(t1).intersection(set(t2)))>=5:\n",
    "\n",
    "                        a = p1[:, m]\n",
    "                        b = p2[:, n]\n",
    "\n",
    "                        # add to similarity score\n",
    "                        similarity_score += scipy.stats.pearsonr(a, b)[0]\n",
    "                        sums += 1\n",
    "\n",
    "            similarity_score /= max([sums, 1])\n",
    "\n",
    "            # add similarity score to the matrix\n",
    "            similarity_matrix[j, k] += similarity_score\n",
    "\n",
    "# take the mean of the scores\n",
    "similarity_matrix /= iterations\n",
    "\n",
    "# plot the scores\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(similarity_matrix)\n",
    "\n",
    "# to get the numbers on the heatmap\n",
    "for i, channel1 in enumerate(channel_list):\n",
    "    for j, channel2 in enumerate(channel_list):\n",
    "\n",
    "        text = ax.text(j, i, round(similarity_matrix[i,j], 2), ha = 'center', va = 'center', color = 'w')\n",
    "\n",
    "ax.set_xticks(np.arange(6), labels = channel_list, rotation = 45)\n",
    "ax.set_yticks(np.arange(6), labels = channel_list)\n",
    "ax.tick_params(left = False, bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare anchored topic models trained on each channel with Pearson similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "iterations = 30\n",
    "similarity_matrix = np.zeros((6, 6, 6))\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    for j, channel1 in enumerate(channel_list):\n",
    "\n",
    "        # load in text and create topic model for channel 1\n",
    "        text1 = pickle.load(open('2022_text_split.pkl', 'rb'))[j]\n",
    "        vec1, tm1 = irulan.tm(text1, anchors = [['covid'], ['ukraine'], ['election'], ['flood'], ['queen'], ['sport']], seed = i)\n",
    "\n",
    "        # get topics into a nice format\n",
    "        topics1 = [[line[k][0] for k in range(len(line))] for line in tm1.get_topics(n_words = 100)]\n",
    "\n",
    "        for k, channel2 in enumerate(channel_list):\n",
    "\n",
    "            # load in text and create topic model for channel 2\n",
    "            text2 = pickle.load(open('2022_text_split.pkl', 'rb'))[k]\n",
    "            vec2, tm2 = irulan.tm(text2, anchors = [['covid'], ['ukraine'], ['election'], ['flood'], ['queen'], ['sport']], seed = i)\n",
    "\n",
    "            # get topics into a nice format\n",
    "            topics2 = [[line[k][0] for k in range(len(line))] for line in tm2.get_topics(n_words = 100)]\n",
    "\n",
    "            # predict values for channel 2s text from channel 1s topic model and vice versa\n",
    "            matrix1 = vec1.transform(text2)\n",
    "            p1 = tm1.transform(matrix1, details = True)[0]\n",
    "\n",
    "            p2 = tm2.p_y_given_x\n",
    "\n",
    "            # initialise the similarity score\n",
    "            similarity_score = 0\n",
    "\n",
    "            # loop through topics \n",
    "            for m in range(6):\n",
    "\n",
    "                a = p1[:, m]\n",
    "                b = p2[:, m]\n",
    "\n",
    "                # add to similarity score\n",
    "                similarity_score = scipy.stats.pearsonr(a, b)[0]\n",
    "\n",
    "                # add similarity score to the matrix\n",
    "                similarity_matrix[j, k, m] += similarity_score\n",
    "\n",
    "        if i == 0:\n",
    "            # print out the contents of the sport topic\n",
    "            print(tm1.get_topics(n_words=40)[5])\n",
    "\n",
    "\n",
    "# take the mean of the scores\n",
    "similarity_matrix /= iterations\n",
    "average_similarity = np.mean(similarity_matrix, axis = 2)\n",
    "\n",
    "# plot the scores\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(similarity_matrix)\n",
    "\n",
    "# to get the numbers on the heatmap\n",
    "for i, channel1 in enumerate(channel_list):\n",
    "    for j, channel2 in enumerate(channel_list):\n",
    "\n",
    "        text = ax.text(j, i, round(similarity_matrix[i,j], 2), ha = 'center', va = 'center', color = 'w')\n",
    "\n",
    "ax.set_xticks(np.arange(6), labels = channel_list, rotation = 45)\n",
    "ax.set_yticks(np.arange(6), labels = channel_list)\n",
    "ax.tick_params(left = False, bottom = False)\n",
    "\n",
    "# create multiple heatmaps - one for each topic\n",
    "fig = plt.figure()\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols=(3, 2),\n",
    "                axes_pad=0.5,\n",
    "                share_all=True,\n",
    "                label_mode=\"L\",\n",
    "                cbar_location=\"right\",\n",
    "                cbar_mode=\"single\",\n",
    "                )\n",
    "\n",
    "# plot a heatmap for each channel \n",
    "for k, topic, ax in zip(range(6), ['covid', 'ukraine', 'election', 'flood', 'queen', 'sport'], grid):\n",
    "\n",
    "    im = ax.imshow(similarity_matrix[:, :, k], vmin = 0.4, vmax = 1)\n",
    "\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "\n",
    "            text = ax.text(j, i, round(similarity_matrix[i,j, k], 2), ha = 'center', va = 'center', color = 'w')\n",
    "    \n",
    "    grid.cbar_axes[0].colorbar(im)\n",
    "\n",
    "    for cax in grid.cbar_axes:\n",
    "        cax.toggle_label(True)\n",
    "\n",
    "    ax.set_title(f\"`{topic}' topic\")\n",
    "    ax.set_xticks(np.arange(6), labels = channel_list, rotation  = 45)\n",
    "    ax.set_yticks(np.arange(6), labels = channel_list)\n",
    "\n",
    "    ax.tick_params(left = False, bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hierarchical topic model to combine sport topics into one larger overarching sport topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pickle.load(open('2022_text_split.pkl', 'rb'))\n",
    "\n",
    "for ch, t in zip(channel_list, text):\n",
    "\n",
    "    vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "    # get sparse matrix of number of times each word appears in each document\n",
    "    matrix = vectorizer.fit_transform(t)\n",
    "\n",
    "    # get the list of words\n",
    "    words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "    # train the CorEx topic model\n",
    "    tm1 = ct.Corex(n_hidden = 200, words = words, seed = 0) \n",
    "    tm1.fit(matrix, words = words)\n",
    "\n",
    "    # train the second topic model on the first topic model's topics\n",
    "    tm2 = ct.Corex(n_hidden = 20, words = words, seed = 0)\n",
    "    tm2.fit(scipy.sparse.csr_matrix(tm1.labels))\n",
    "\n",
    "    # get the content of the first topic model's topics\n",
    "    tm1_topics = tm1.get_topics()\n",
    "\n",
    "    for topic in tm2.get_topics():\n",
    "\n",
    "        # initialise a list containing all of the topic words, and a list separated into the smaller topics\n",
    "        topic_list = list()\n",
    "        topic_list_sep = list()\n",
    "\n",
    "        # create lists by looping through topics\n",
    "        for t in topic:\n",
    "            topic_list.extend([tm1_topics[t[0]][i][0] for i in range(len(tm1_topics[t[0]]))])\n",
    "            topic_list_sep.append([tm1_topics[t[0]][i][0] for i in range(len(tm1_topics[t[0]]))])\n",
    "\n",
    "        # check if sport-related terms are in any of the topics - if so, this is a larger sport topic and we will print its contents\n",
    "        if 'sport' in topic_list or 'ball' in topic_list or 'game' in topic_list or 'player' in topic_list:\n",
    "\n",
    "            print(topic_list_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot media attention for supervised topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "iterations = 30\n",
    "a = 5000\n",
    "mean_pyx = [np.zeros((len(text), 40))]*6\n",
    "anchors = [['covid'], ['ukraine'], ['election'], ['flood'], ['queen'], ['sport']]\n",
    "\n",
    "for j, channel in enumerate(channel_list):\n",
    "\n",
    "    # load in text and create topic model for channel 1\n",
    "    text = pickle.load(open('2022_all_clean_split.pkl', 'rb'))[j]\n",
    "    dates = pickle.load(open('2022_all_clean_dates_split.pkl','rb'))[i]\n",
    "    \n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # make a topic model for each iteration\n",
    "        vec, tm = irulan.tm(text, anchors = anchors, seed = i)\n",
    "\n",
    "        mean_pyx[j] += tm.p_y_given_x\n",
    "\n",
    "    # take the mean of the p(y|x) terms\n",
    "    mean_pyx[j] /= iterations\n",
    "\n",
    "# plot for each topic\n",
    "for t, topic in enumerate(anchors):\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    for j, channel in enumerate(channel_list):\n",
    "    \n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(mean_pyx[j][:, t]), label = channel)\n",
    "\n",
    "    plt.title(f\"{topic} topic\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "    plt.legend(loc = 'upper left', bbox_to_anchor = (1,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coverage bias for some topics using a topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['covid', 'ukraine', 'election', 'flood', 'queen', 'sport']\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    # load in text \n",
    "    text = pickle.load(open('all_clean_split.pkl', 'rb'))[i]\n",
    "\n",
    "    # get all counts of the text\n",
    "    vecs = CountVectorizer()\n",
    "    counts = vecs.fit_transform(text[i])\n",
    "\n",
    "\n",
    "    for word in word_list:\n",
    "\n",
    "        # find the counts of the words in the word list and print\n",
    "        s = (np.sum(counts[:, vecs.vocabulary_[word]]))/len(text)\n",
    "        print(f'{channel}, {word}: {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coverage bias for some topics using a topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to getting media attention\n",
    "\n",
    "# initialise\n",
    "iterations = 30\n",
    "a = 5000\n",
    "mean_pyx = [np.zeros((len(text), 40))]*6\n",
    "anchors = [['covid'], ['ukraine'], ['election'], ['flood'], ['queen'], ['sport']]\n",
    "bias_matrix = np.zeros((6, 6))\n",
    "\n",
    "for j, channel in enumerate(channel_list):\n",
    "\n",
    "        # load in text and create topic model for channel 1\n",
    "        text = pickle.load(open('all_text_split.pkl', 'rb'))[j]\n",
    "        dates = pickle.load(open('all_dates_split.pkl','rb'))[i]\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            # subsample text\n",
    "            np.random.seed(i)\n",
    "            p = 0.1\n",
    "            ints = np.random.randint(0, len(text), int(len(text)*p))\n",
    "            subsampled_text = [subsampled_text[i] for i in ints]\n",
    "\n",
    "            # train topic model with anchors\n",
    "            vec, tm = irulan.tm(text, anchors = anchors, seed = i)\n",
    "\n",
    "            # add to the mean p(y|x)\n",
    "            mean_pyx[j] += tm.p_y_given_x\n",
    "\n",
    "        # take the mean of each channel's p(y|x)\n",
    "        mean_pyx[j] /= iterations\n",
    "\n",
    "        # make predictions from the sampled topic models\n",
    "        matrix = vec.transform(text)\n",
    "        pyx = tm.transform(matrix, details = True)[0]\n",
    "\n",
    "        for t, topic in enumerate(anchors):\n",
    "\n",
    "            bias = np.mean(mean_pyx[j][:, t])\n",
    "            bias_matrix[j, t] = bias\n",
    "\n",
    "            print(f'Coverage bias for {topic} topic on {channel}:', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate political coverage bias using word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists = [['liberal', 'scott', 'morrison', 'malcolm', 'turnbull'], ['labor', 'anthony', 'albanese', 'bill', 'shorten']]\n",
    "\n",
    "for i, channel in enumerate(channel_list):\n",
    "\n",
    "    # load in text \n",
    "    text = pickle.load(open('all_text_split.pkl', 'rb'))[i]\n",
    "\n",
    "    # get all counts of the text\n",
    "    vecs = CountVectorizer()\n",
    "    counts = vecs.fit_transform(text[i])\n",
    "\n",
    "\n",
    "    for party in word_lists:\n",
    "\n",
    "        s = 0\n",
    "\n",
    "        for word in party:\n",
    "\n",
    "            # find the counts of the words in the word list and print\n",
    "            s += (np.sum(counts[:, vecs.vocabulary_[word]]))\n",
    "\n",
    "        print(f'{channel}, {word}: {s/len(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate political coverage bias using a topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to getting media attention\n",
    "\n",
    "# initialise\n",
    "iterations = 30\n",
    "a = 5000\n",
    "mean_pyx = [np.zeros((len(text), 40))]*6\n",
    "anchors = [['liberal', 'scott', 'morrison', 'malcolm', 'turnbull'], ['labor', 'anthony', 'albanese', 'bill', 'shorten']]\n",
    "bias_matrix = np.zeros((6, 6))\n",
    "mod = np.zeros(6)\n",
    "ros = np.zeros(6)\n",
    "\n",
    "for j, channel in enumerate(channel_list):\n",
    "\n",
    "        # load in text and create topic model for channel 1\n",
    "        text = pickle.load(open('all_text_split.pkl', 'rb'))[j]\n",
    "        dates = pickle.load(open('all_dates_split.pkl','rb'))[i]\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "            # subsample text\n",
    "            np.random.seed(i)\n",
    "            p = 0.1\n",
    "            ints = np.random.randint(0, len(text), int(len(text)*p))\n",
    "            subsampled_text = [subsampled_text[i] for i in ints]\n",
    "\n",
    "            # train topic model with anchors\n",
    "            vec, tm = irulan.tm(text, anchors = anchors, seed = i)\n",
    "\n",
    "            # make predictions from the sampled topic models\n",
    "            matrix = vec.transform(text)\n",
    "            pyx = tm.transform(matrix, details = True)[0]\n",
    "\n",
    "            # add to the mean p(y|x)\n",
    "            mean_pyx[j] += pyx\n",
    "\n",
    "        # take the mean of each channel's p(y|x)\n",
    "        mean_pyx[j] /= iterations\n",
    "\n",
    "        for t, party in enumerate(anchors):\n",
    "\n",
    "            bias = np.mean(mean_pyx[j][:, t])\n",
    "            bias_matrix[j, t] = bias\n",
    "\n",
    "            print(f'Coverage bias for {party[0]} topic on {channel}:', bias)\n",
    "\n",
    "        # calculate MOD\n",
    "        mod[j] = np.mean(mean_pyx[j][:, 0] - mean_pyx[j][:, 1])\n",
    "        print(f'MOD for {channel}:', bias)\n",
    "\n",
    "        # caluclate ROS\n",
    "        ros[j] = np.sum(mean_pyx[j][:, 0])/np.sum(mean_pyx[j][:, 1])\n",
    "        print(f'ROS for {channel}:', bias)\n",
    "\n",
    "# save the mean of the p(y|x) terms\n",
    "pickle.dump(mean_pyx, open('political_probabilities.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
