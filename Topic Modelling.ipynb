{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # to get \"unresolved\" packages\n",
    "sys.path.append(\"/Users/a1765262/opt/anaconda3/lib/python3.9/site-packages\")\n",
    "\n",
    "import pandas as pd # for data frames\n",
    "import os # for working directories\n",
    "import numpy as np # for numbers\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from corextopic import corextopic as ct # for topic model\n",
    "import string # for preprocessing\n",
    "import sklearn.feature_extraction.text # for vectoriser\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # for sentiment\n",
    "import pickle\n",
    "import scipy\n",
    "from nltk import FreqDist\n",
    "import shifterator as sh\n",
    "import scipy.stats as st\n",
    "import datetime\n",
    "# from readability import readability\n",
    "\n",
    "import irulan\n",
    "\n",
    "channel_list = [\"ABC1\", \"Ch7\", \"Ch9\", \"Ch10\", \"SBS\", \"ABC24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a topic model anchored on 'sport' for different document lengths. Plot the number of words in each document for 5-minute and program lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector of seconds that we will look at (9999 seconds represents the program split)\n",
    "seconds = [10, 60, 120, 300, 600, 1200, 1800, 9999]\n",
    "\n",
    "abc1_2022 = pd.read_csv(\"ABC1_2022.csv\")\n",
    "\n",
    "for i, s in enumerate(seconds):\n",
    "\n",
    "    # clean text so that documents are s seconds long\n",
    "    text = irulan.clean_split_mins(abc1_2022, s)\n",
    "\n",
    "    # train topic model\n",
    "    _, tm = irulan.tm(text, anchors = ['sport'], num_topics= 40)\n",
    "\n",
    "    print(tm.get_topics()[0])\n",
    "\n",
    "    # if we have split into 5-minute or program-length intervals, make histogram\n",
    "    if s in [60, 9999]:\n",
    "        plt.hist([len(doc.split()) for doc in text], bins = 50)\n",
    "        plt.title(\"The number of words contained in each program\")\n",
    "        plt.xlabel(\"Number of words\")\n",
    "        plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total correlation of topic models with the number of topics adjusted from 10 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pickle.load(open(\"2022_all_clean.pkl\", \"rb\"))\n",
    "\n",
    "# make the topic model\n",
    "n_topics = np.array(range(10, 110, 10))\n",
    "iterations = 30\n",
    "tcs = np.empty((len(n_topics), iterations))\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "for k, n in enumerate(n_topics):\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # train the topic model\n",
    "        tm = ct.Corex(n_hidden = n, words = words, max_iter = 400, seed = i) \n",
    "        tm.fit(matrix, words = words, anchor_strength = 10)\n",
    "\n",
    "        tcs[k, i] = tm.tc\n",
    "\n",
    "        # save the topic model for later\n",
    "        pickle.dump(open(f'./{n}_topics_{i}.pkl', 'wb'))\n",
    "\n",
    "        print(f'Value {i} for {n} topics is: {tm.tc}')\n",
    "        del tm\n",
    "\n",
    "# make violin plot\n",
    "plt.violinplot(tcs, positions = np.array(range(10, 110, 10)), widths = 5)\n",
    "plt.title(\"The total correlation of topic models on 2022 data adjusting the number of topics\")\n",
    "plt.ylabel(\"Total correlation\")\n",
    "plt.xlabel(\"Number of topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson and Spearman similarities for topic models with an adjacent number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_similarity(tm1, tm2, similarity_type = 'pearson'):\n",
    "\n",
    "    assert similarity_type in ['pearson', 'spearman'], \"'similarity_type' should be 'pearson' or 'spearman'.\"\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    pyx1 = tm1.p_y_given_x\n",
    "    pyx2 = tm2.p_y_given_x\n",
    "\n",
    "    topics1 = [[line[k][0] for k in range(len(line))] for line in tm1.get_topics(n_words = 100)]\n",
    "    topics2 = [[line[k][0] for k in range(len(line))] for line in tm2.get_topics(n_words = 100)]\n",
    "\n",
    "    similarity_score = 0\n",
    "\n",
    "    for i, t1 in enumerate(topics1):\n",
    "            for j, t2 in enumerate(topics2):\n",
    "\n",
    "                if len(set(t1).intersection(set(t2)))>=5:\n",
    "\n",
    "                    a = pyx1[:, i]\n",
    "\n",
    "                    b = pyx2[:, j]\n",
    "\n",
    "                    if similarity_type == 'pearson':\n",
    "\n",
    "                        similarity_score += scipy.stats.pearsonr(a, b)[0]\n",
    "                        k += 1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        similarity_score += scipy.stats.spearmanr(a, b, nan_policy='omit')[0]\n",
    "                        k += 1\n",
    "    \n",
    "    return similarity_score/max([k, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we already have 30 topic models for each number of topics...\n",
    "\n",
    "piecewise_comparisons = zip(range(10, 110, 10), range(20, 100, 10))\n",
    "iterations = 30\n",
    "pearson_matrix = np.identity(10)*iterations\n",
    "spearman_matrix = np.identity(10)*iterations\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    for n1, n2 in piecewise_comparisons:\n",
    "\n",
    "        tm1 = pickle.load(open(f'./{n}_topics_{i}.pkl', 'rb'))\n",
    "        tm2 = pickle.load(open(f'./{n}_topics_{i}.pkl', 'rb'))\n",
    "\n",
    "        pearson = topic_similarity(tm1, tm2, 'pearson')\n",
    "        pearson_matrix[n1, n2] += pearson\n",
    "        pearson_matrix[n2, n1] += pearson\n",
    "\n",
    "        spearman = topic_similarity(tm1, tm2, 'spearman')\n",
    "        spearman_matrix[n1, n2] += spearman\n",
    "        spearman_matrix[n2, n1] += spearman\n",
    "\n",
    "pearson_matrix /= iterations\n",
    "spearman_matrix /= iterations\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "\n",
    "        if i!=j and i+1!=j and i-1!=j:\n",
    "            spearman_matrix[i,j] = np.nan\n",
    "            pearson_matrix[i,j] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(spearman_matrix, vmin = 0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "\n",
    "        text = ax.text(j, i, round(spearman_matrix[i,j], 2), ha = 'center', va = 'center', color = 'w')\n",
    "\n",
    "ax.set_xticks(np.arange(10), labels = np.arange(10, 110, 10))\n",
    "ax.set_yticks(np.arange(10), labels = np.arange(10, 110, 10))\n",
    "ax.set_xlabel(\"Number of topics\")\n",
    "ax.set_ylabel(\"Number of topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate unsupervised topic models trained on 2022 data from ABC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open(\"2022_all_clean_split.pkl\", \"rb\"))[0]\n",
    "dates = pickle.load(open(\"2022_all_dates_split.pkl\", \"rb\"))[0]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 5\n",
    "a = 5000\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # train the topic model\n",
    "    tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "    tm.fit(matrix, words = words)\n",
    "    print(tm.get_topics()[:5])\n",
    "\n",
    "    # sport will be topic 0 for all topic models except the second one\n",
    "    if i != 1:\n",
    "\n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:, 0], a), alpha = 0.8, label = i+1)\n",
    "\n",
    "    else:\n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:, 4], a), alpha = 0.8, label = i+1)\n",
    "\n",
    "    del tm\n",
    "\n",
    "# make the plot nice\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 30 topic models on ABC1 text with the anchor 'sport'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open('2022_all_clean_split.pkl', 'rb'))[0]\n",
    "dates = pickle.load(open('2022_all_dates_split.pkl', 'rb'))[0]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 30\n",
    "a = 5000\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # train the topic model\n",
    "    tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "    tm.fit(matrix, words = words, anchors = ['sport'], anchor_strength = 10)\n",
    "    print(tm.get_topics()[0])\n",
    "    plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm.p_y_given_x[:, 0], a), color = 'black', alpha = 0.5)\n",
    "    del tm\n",
    "\n",
    "plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 30 topic models with anchors for each method of inference and calculate Pearson correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open('2022_all_clean_split.pkl', 'rb'))[0]\n",
    "dates = pickle.load(open('2022_all_dates_split.pkl', 'rb'))[0]\n",
    "anchors = [['covid'],['sport'],['gardening'],['election']]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 30\n",
    "a = 5000\n",
    "correlations = np.zeros((2, 30))\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # ground truth\n",
    "    tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "    tm.fit(matrix, words = words, anchors = anchors, anchor_strength = 10)\n",
    "\n",
    "    # method 1\n",
    "    text_sampled = [text[i] for i in range(len(text)) if dates[i].year == 2022]\n",
    "    vec1, tm1 = irulan.tm(text_sampled, num_topics=40, anchors = anchors, seed = i)\n",
    "\n",
    "    # method 2\n",
    "    np.random.seed(seed = i)\n",
    "\n",
    "    # subsample proportion p\n",
    "    p = 0.2\n",
    "    ints = np.random.randint(0, len(text), int(len(text)*p))\n",
    "    text_sampled = [text[i] for i in ints]\n",
    "\n",
    "    vec2, tm2 = irulan.tm(text_sampled, num_topics=40, anchors = anchors, seed = i)\n",
    "\n",
    "    # make predictions from the sampled topic models\n",
    "    matrix2 = vec1.transform(text)\n",
    "    p1 = tm1.transform(matrix2, details = True)[0]\n",
    "\n",
    "    matrix1 = vec2.transform(text)\n",
    "    p2 = tm2.transform(matrix1, details = True)[0]\n",
    "\n",
    "    correlation1 = scipy.stats.pearsonr(tm.p_y_given_x[:, 0], p1[:, 0])\n",
    "    correlation2 = scipy.stats.pearsonr(tm.p_y_given_x[:, 0], p2[:, 0])\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(dates[int(a/2):-int(a/2)+1], irulan.moving_average(tm.p_y_given_x[:, 0], a), label = 'Ground truth')\n",
    "        plt.plot(dates[int(a/2):-int(a/2)+1], irulan.moving_average(p1[:, 0], a), label = 'Method 1')\n",
    "        plt.plot(dates[int(a/2):-int(a/2)+1], irulan.moving_average(p2[:, 0], a), label = 'Method 2')\n",
    "\n",
    "        for k, anchor in enumerate(anchors[0]):\n",
    "            print(f'{anchor} Method 1: {correlation1}')\n",
    "            print(f'{anchor} Method 2: {correlation2}')\n",
    "\n",
    "    correlations[0, i] = correlation1\n",
    "    correlations[1, i] = correlation2\n",
    "\n",
    "    correlation_means = np.mean(correlations, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different percentages to subsample with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text and dates from ABC1\n",
    "text = pickle.load(open('2022_all_clean_split.pkl', 'rb'))[0]\n",
    "dates = pickle.load(open('2022_all_dates_split.pkl', 'rb'))[0]\n",
    "anchors = [['covid'],['sport'],['gardening'],['election']]\n",
    "subsampled_proportions = [0.001, 0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# initialise vectorizer \n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "# get sparse matrix of number of times each word appears in each document\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "# get the list of words\n",
    "words = sorted(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# make the topic model\n",
    "n_topics = 40\n",
    "iterations = 30\n",
    "a = 5000\n",
    "correlations = np.zeros((30, len(subsampled_proportions)))\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    for p in subsampled_proportions:\n",
    "\n",
    "        # ground truth\n",
    "        tm = ct.Corex(n_hidden = n_topics, words = words, max_iter = 400, seed = i) \n",
    "        tm.fit(matrix, words = words, anchors = anchors, anchor_strength = 10)\n",
    "\n",
    "        # method 2\n",
    "        np.random.seed(seed = i)\n",
    "\n",
    "        # subsample proportion p\n",
    "        ints = np.random.randint(0, len(text), int(len(text)*p))\n",
    "        text_sampled = [text[i] for i in ints]\n",
    "\n",
    "        vec2, tm2 = irulan.tm(text_sampled, num_topics=40, anchors = anchors, seed = i)\n",
    "\n",
    "        # make predictions from the sampled topic model\n",
    "        matrix1 = vec2.transform(text)\n",
    "        p2 = tm2.transform(matrix1, details = True)[0]\n",
    "\n",
    "        correlations[i, p] = scipy.stats.pearsonr(tm.p_y_given_x[:, 0], p2[:, 0])\n",
    "\n",
    "plt.violinplot(correlations);\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9], [\"0.1\", \"1\", \"2\", \"5\", \"10\", \"20\", \"40\", \"60\", \"80\"])\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.xlabel(\"Percentage of subsample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate unsupervised topic models trained on 2022 data for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, channel in channel_list:\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # read in text and dates\n",
    "    text = pickle.load(open('2022_all_clean_split.pkl', 'rb'))[i]\n",
    "    dates = pickle.load(open('2022_all_dates_split.pkl', 'rb'))[i]\n",
    "\n",
    "    # train topic model\n",
    "    _, tm2 = irulan.tm(text_sampled, num_topics=40)\n",
    "\n",
    "    # print topics\n",
    "    print(tm.get_topics()[:5])\n",
    "\n",
    "    for j in range(5):\n",
    "        plt.plot(dates[int(a/2):1-int(a/2)], irulan.moving_average(tm2.p_y_given_x[:,j], a))\n",
    "\n",
    "    plt.xticks([datetime.date(year = 2022, month = 2, day = 1), datetime.date(year = 2022, month = 5, day = 1), datetime.date(year = 2022, month = 8, day = 1), datetime.date(year = 2022, month = 11, day = 1)])\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend(bbox_to_anchor = (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare unsupervised topic models trained on each channel with Pearson similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare anchored topic models trained on each channel with Pearson similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hierarchical topic model to combine sport topics into one larger overarching sport topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot media attention for supervised topic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coverage bias for some topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get political word counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate political coverage bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
